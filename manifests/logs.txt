
==> Audit <==
|---------|--------------------|----------|-----------------|---------|---------------------|---------------------|
| Command |        Args        | Profile  |      User       | Version |     Start Time      |      End Time       |
|---------|--------------------|----------|-----------------|---------|---------------------|---------------------|
| start   | --vm-driver hyperv | minikube | HEY-AMIGO\pusto | v1.34.0 | 09 Dec 24 12:25 CST | 09 Dec 24 12:30 CST |
| addons  | enable ingress     | minikube | HEY-AMIGO\pusto | v1.34.0 | 09 Dec 24 12:31 CST | 09 Dec 24 12:32 CST |
| ip      |                    | minikube | HEY-AMIGO\pusto | v1.34.0 | 09 Dec 24 12:34 CST | 09 Dec 24 12:34 CST |
| service | kubernetes-service | minikube | HEY-AMIGO\pusto | v1.34.0 | 09 Dec 24 12:46 CST |                     |
|---------|--------------------|----------|-----------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/12/09 12:25:08
Running on machine: hey-amigo
Binary: Built with gc go1.22.5 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1209 12:25:08.105350   22636 out.go:345] Setting OutFile to fd 112 ...
I1209 12:25:08.106358   22636 out.go:358] Setting ErrFile to fd 116...
W1209 12:25:08.125426   22636 root.go:314] Error reading config file at C:\Users\pusto\.minikube\config\config.json: open C:\Users\pusto\.minikube\config\config.json: The system cannot find the path specified.
I1209 12:25:08.134871   22636 out.go:352] Setting JSON to false
I1209 12:25:08.141911   22636 start.go:129] hostinfo: {"hostname":"hey-amigo","uptime":115162,"bootTime":1733653545,"procs":295,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.2454 Build 26100.2454","kernelVersion":"10.0.26100.2454 Build 26100.2454","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"c4df9727-d30e-4e62-9c43-050e541d7bcf"}
W1209 12:25:08.141911   22636 start.go:137] gopshost.Virtualization returned error: not implemented yet
I1209 12:25:08.144105   22636 out.go:177] 😄  minikube v1.34.0 on Microsoft Windows 11 Pro 10.0.26100.2454 Build 26100.2454
I1209 12:25:08.145751   22636 notify.go:220] Checking for updates...
W1209 12:25:08.146402   22636 preload.go:293] Failed to list preload files: open C:\Users\pusto\.minikube\cache\preloaded-tarball: The system cannot find the file specified.
I1209 12:25:08.146939   22636 driver.go:394] Setting default libvirt URI to qemu:///system
I1209 12:25:13.372315   22636 out.go:177] ✨  Using the hyperv driver based on user configuration
I1209 12:25:13.374450   22636 start.go:297] selected driver: hyperv
I1209 12:25:13.374450   22636 start.go:901] validating driver "hyperv" against <nil>
I1209 12:25:13.374450   22636 start.go:912] status for hyperv: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1209 12:25:13.375133   22636 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I1209 12:25:13.452983   22636 start_flags.go:393] Using suggested 6000MB memory alloc based on sys=32474MB, container=0MB
I1209 12:25:13.453573   22636 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I1209 12:25:13.453573   22636 cni.go:84] Creating CNI manager for ""
I1209 12:25:13.453573   22636 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1209 12:25:13.453573   22636 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1209 12:25:13.453573   22636 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:6000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\pusto:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1209 12:25:13.454870   22636 iso.go:125] acquiring lock: {Name:mkfa62a8ff58656bd6a680642bd8f54ad2e122c4 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1209 12:25:13.456276   22636 out.go:177] 💿  Downloading VM boot image ...
I1209 12:25:13.458251   22636 download.go:107] Downloading: https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso?checksum=file:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso.sha256 -> C:\Users\pusto\.minikube\cache\iso\amd64\minikube-v1.34.0-amd64.iso
I1209 12:26:09.312514   22636 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I1209 12:26:09.314439   22636 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1209 12:26:09.462171   22636 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.31.0/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1209 12:26:09.462171   22636 cache.go:56] Caching tarball of preloaded images
I1209 12:26:09.462746   22636 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1209 12:26:09.465784   22636 out.go:177] 💾  Downloading Kubernetes v1.31.0 preload ...
I1209 12:26:09.466966   22636 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 ...
I1209 12:26:09.922902   22636 download.go:107] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.31.0/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4?checksum=md5:2dd98f97b896d7a4f012ee403b477cc8 -> C:\Users\pusto\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1209 12:27:01.778690   22636 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 ...
I1209 12:27:01.782687   22636 preload.go:254] verifying checksum of C:\Users\pusto\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 ...
I1209 12:27:02.723150   22636 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1209 12:27:02.723676   22636 profile.go:143] Saving config to C:\Users\pusto\.minikube\profiles\minikube\config.json ...
I1209 12:27:02.724215   22636 lock.go:35] WriteFile acquiring C:\Users\pusto\.minikube\profiles\minikube\config.json: {Name:mkda8c59ea4f08c82a237733b74e20581572a080 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 12:27:02.725874   22636 start.go:360] acquireMachinesLock for minikube: {Name:mke93f897e44d96d263467161cb4401bf3a9a863 Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I1209 12:27:02.725874   22636 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I1209 12:27:02.725874   22636 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:6000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\pusto:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1209 12:27:02.725874   22636 start.go:125] createHost starting for "" (driver="hyperv")
I1209 12:27:02.730188   22636 out.go:235] 🔥  Creating hyperv VM (CPUs=2, Memory=6000MB, Disk=20000MB) ...
I1209 12:27:02.731281   22636 start.go:159] libmachine.API.Create for "minikube" (driver="hyperv")
I1209 12:27:02.731281   22636 client.go:168] LocalClient.Create starting
I1209 12:27:02.732373   22636 main.go:141] libmachine: Creating CA: C:\Users\pusto\.minikube\certs\ca.pem
I1209 12:27:03.080869   22636 main.go:141] libmachine: Creating client certificate: C:\Users\pusto\.minikube\certs\cert.pem
I1209 12:27:03.298012   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive @(Get-Module -ListAvailable hyper-v).Name | Get-Unique
I1209 12:27:05.299325   22636 main.go:141] libmachine: [stdout =====>] : Hyper-V

I1209 12:27:05.299325   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:05.299325   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive @([Security.Principal.WindowsPrincipal][Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole(([System.Security.Principal.SecurityIdentifier]::new("S-1-5-32-578")))
I1209 12:27:06.824975   22636 main.go:141] libmachine: [stdout =====>] : False

I1209 12:27:06.824975   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:06.824975   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive @([Security.Principal.WindowsPrincipal][Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole([Security.Principal.WindowsBuiltInRole] "Administrator")
I1209 12:27:08.151791   22636 main.go:141] libmachine: [stdout =====>] : True

I1209 12:27:08.151791   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:08.151791   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive [Console]::OutputEncoding = [Text.Encoding]::UTF8; ConvertTo-Json @(Hyper-V\Get-VMSwitch|Select Id, Name, SwitchType|Where-Object {($_.SwitchType -eq 'External') -or ($_.Id -eq 'c08cb7b8-9b3c-408e-8e30-5e16a3aeb444')}|Sort-Object -Property SwitchType)
I1209 12:27:11.976878   22636 main.go:141] libmachine: [stdout =====>] : [
    {
        "Id":  "c08cb7b8-9b3c-408e-8e30-5e16a3aeb444",
        "Name":  "Default Switch",
        "SwitchType":  1
    }
]

I1209 12:27:11.976878   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:11.978574   22636 main.go:141] libmachine: Downloading C:\Users\pusto\.minikube\cache\boot2docker.iso from file://C:/Users/pusto/.minikube/cache/iso/amd64/minikube-v1.34.0-amd64.iso...
I1209 12:27:12.450286   22636 main.go:141] libmachine: Creating SSH key...
I1209 12:27:12.665702   22636 main.go:141] libmachine: Creating VM...
I1209 12:27:12.665702   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive [Console]::OutputEncoding = [Text.Encoding]::UTF8; ConvertTo-Json @(Hyper-V\Get-VMSwitch|Select Id, Name, SwitchType|Where-Object {($_.SwitchType -eq 'External') -or ($_.Id -eq 'c08cb7b8-9b3c-408e-8e30-5e16a3aeb444')}|Sort-Object -Property SwitchType)
I1209 12:27:16.648409   22636 main.go:141] libmachine: [stdout =====>] : [
    {
        "Id":  "c08cb7b8-9b3c-408e-8e30-5e16a3aeb444",
        "Name":  "Default Switch",
        "SwitchType":  1
    }
]

I1209 12:27:16.648409   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:16.648409   22636 main.go:141] libmachine: Using switch "Default Switch"
I1209 12:27:16.648409   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive @([Security.Principal.WindowsPrincipal][Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole([Security.Principal.WindowsBuiltInRole] "Administrator")
I1209 12:27:18.506008   22636 main.go:141] libmachine: [stdout =====>] : True

I1209 12:27:18.506008   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:18.506008   22636 main.go:141] libmachine: Creating VHD
I1209 12:27:18.506008   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\New-VHD -Path 'C:\Users\pusto\.minikube\machines\minikube\fixed.vhd' -SizeBytes 10MB -Fixed
I1209 12:27:21.908174   22636 main.go:141] libmachine: [stdout =====>] : 

ComputerName            : HEY-AMIGO
Path                    : C:\Users\pusto\.minikube\machines\minikube\fixed.vhd
VhdFormat               : VHD
VhdType                 : Fixed
FileSize                : 10486272
Size                    : 10485760
MinimumSize             : 
LogicalSectorSize       : 512
PhysicalSectorSize      : 512
BlockSize               : 0
ParentPath              : 
DiskIdentifier          : CC96F678-A17F-4A95-B750-E83E7188A4F3
FragmentationPercentage : 0
Alignment               : 1
Attached                : False
DiskNumber              : 
IsPMEMCompatible        : False
AddressAbstractionType  : None
Number                  : 




I1209 12:27:21.908174   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:21.908174   22636 main.go:141] libmachine: Writing magic tar header
I1209 12:27:21.911317   22636 main.go:141] libmachine: Writing SSH key tar header
I1209 12:27:21.926486   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Convert-VHD -Path 'C:\Users\pusto\.minikube\machines\minikube\fixed.vhd' -DestinationPath 'C:\Users\pusto\.minikube\machines\minikube\disk.vhd' -VHDType Dynamic -DeleteSource
I1209 12:27:25.146874   22636 main.go:141] libmachine: [stdout =====>] : 
I1209 12:27:25.146874   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:25.146874   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Resize-VHD -Path 'C:\Users\pusto\.minikube\machines\minikube\disk.vhd' -SizeBytes 20000MB
I1209 12:27:28.422240   22636 main.go:141] libmachine: [stdout =====>] : 
I1209 12:27:28.422240   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:28.422240   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\New-VM minikube -Path 'C:\Users\pusto\.minikube\machines\minikube' -SwitchName 'Default Switch' -MemoryStartupBytes 6000MB
I1209 12:27:36.131025   22636 main.go:141] libmachine: [stdout =====>] : 
Name     State CPUUsage(%) MemoryAssigned(M) Uptime   Status             Version
----     ----- ----------- ----------------- ------   ------             -------
minikube Off   0           0                 00:00:00 Operating normally 12.0   



I1209 12:27:36.131025   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:36.131025   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Set-VMMemory -VMName minikube -DynamicMemoryEnabled $false
I1209 12:27:39.082726   22636 main.go:141] libmachine: [stdout =====>] : 
I1209 12:27:39.082726   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:39.082726   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Set-VMProcessor minikube -Count 2
I1209 12:27:41.800094   22636 main.go:141] libmachine: [stdout =====>] : 
I1209 12:27:41.800094   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:41.800094   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Set-VMDvdDrive -VMName minikube -Path 'C:\Users\pusto\.minikube\machines\minikube\boot2docker.iso'
I1209 12:27:44.945947   22636 main.go:141] libmachine: [stdout =====>] : 
I1209 12:27:44.945947   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:44.945947   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Add-VMHardDiskDrive -VMName minikube -Path 'C:\Users\pusto\.minikube\machines\minikube\disk.vhd'
I1209 12:27:47.914226   22636 main.go:141] libmachine: [stdout =====>] : 
I1209 12:27:47.914226   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:47.914226   22636 main.go:141] libmachine: Starting VM...
I1209 12:27:47.914226   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Start-VM minikube
I1209 12:27:51.497305   22636 main.go:141] libmachine: [stdout =====>] : 
I1209 12:27:51.497305   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:51.497305   22636 main.go:141] libmachine: Waiting for host to start...
I1209 12:27:51.497305   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:27:54.628009   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:27:54.628009   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:54.628009   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:27:58.183393   22636 main.go:141] libmachine: [stdout =====>] : 
I1209 12:27:58.183393   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:27:59.191660   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:28:01.836340   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:28:01.836340   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:01.836340   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:28:05.608863   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:28:05.608863   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:05.608863   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:28:08.956126   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:28:08.956126   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:08.956126   22636 machine.go:93] provisionDockerMachine start ...
I1209 12:28:08.956126   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:28:12.054278   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:28:12.054278   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:12.054278   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:28:14.896475   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:28:14.896475   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:14.898552   22636 main.go:141] libmachine: Using SSH client type: native
I1209 12:28:14.921302   22636 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8cc9c0] 0x8cf5a0 <nil>  [] 0s} 172.17.230.63 22 <nil> <nil>}
I1209 12:28:14.921302   22636 main.go:141] libmachine: About to run SSH command:
hostname
I1209 12:28:15.034335   22636 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1209 12:28:15.034335   22636 buildroot.go:166] provisioning hostname "minikube"
I1209 12:28:15.034335   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:28:17.198470   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:28:17.198470   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:17.198470   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:28:20.762368   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:28:20.762368   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:20.766341   22636 main.go:141] libmachine: Using SSH client type: native
I1209 12:28:20.766341   22636 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8cc9c0] 0x8cf5a0 <nil>  [] 0s} 172.17.230.63 22 <nil> <nil>}
I1209 12:28:20.766341   22636 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1209 12:28:20.920509   22636 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1209 12:28:20.920509   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:28:23.676715   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:28:23.676715   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:23.676715   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:28:26.763954   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:28:26.763954   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:26.767871   22636 main.go:141] libmachine: Using SSH client type: native
I1209 12:28:26.767871   22636 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8cc9c0] 0x8cf5a0 <nil>  [] 0s} 172.17.230.63 22 <nil> <nil>}
I1209 12:28:26.767871   22636 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1209 12:28:26.890355   22636 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1209 12:28:26.890355   22636 buildroot.go:172] set auth options {CertDir:C:\Users\pusto\.minikube CaCertPath:C:\Users\pusto\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\pusto\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\pusto\.minikube\machines\server.pem ServerKeyPath:C:\Users\pusto\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\pusto\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\pusto\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\pusto\.minikube}
I1209 12:28:26.890355   22636 buildroot.go:174] setting up certificates
I1209 12:28:26.890355   22636 provision.go:84] configureAuth start
I1209 12:28:26.890861   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:28:29.510291   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:28:29.510291   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:29.510291   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:28:33.160711   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:28:33.160711   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:33.160711   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:28:36.135329   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:28:36.135329   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:36.135329   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:28:39.904718   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:28:39.905494   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:39.905494   22636 provision.go:143] copyHostCerts
I1209 12:28:39.905494   22636 exec_runner.go:151] cp: C:\Users\pusto\.minikube\certs\ca.pem --> C:\Users\pusto\.minikube/ca.pem (1074 bytes)
I1209 12:28:39.906666   22636 exec_runner.go:151] cp: C:\Users\pusto\.minikube\certs\cert.pem --> C:\Users\pusto\.minikube/cert.pem (1119 bytes)
I1209 12:28:39.907288   22636 exec_runner.go:151] cp: C:\Users\pusto\.minikube\certs\key.pem --> C:\Users\pusto\.minikube/key.pem (1679 bytes)
I1209 12:28:39.908389   22636 provision.go:117] generating server cert: C:\Users\pusto\.minikube\machines\server.pem ca-key=C:\Users\pusto\.minikube\certs\ca.pem private-key=C:\Users\pusto\.minikube\certs\ca-key.pem org=pusto.minikube san=[127.0.0.1 172.17.230.63 localhost minikube]
I1209 12:28:40.090188   22636 provision.go:177] copyRemoteCerts
I1209 12:28:40.091193   22636 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1209 12:28:40.091193   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:28:42.197556   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:28:42.197556   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:42.197556   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:28:45.165088   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:28:45.165088   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:45.165088   22636 sshutil.go:53] new ssh client: &{IP:172.17.230.63 Port:22 SSHKeyPath:C:\Users\pusto\.minikube\machines\minikube\id_rsa Username:docker}
I1209 12:28:45.279591   22636 ssh_runner.go:235] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (5.1883984s)
I1209 12:28:45.280173   22636 ssh_runner.go:362] scp C:\Users\pusto\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1209 12:28:45.320977   22636 ssh_runner.go:362] scp C:\Users\pusto\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1209 12:28:45.352029   22636 ssh_runner.go:362] scp C:\Users\pusto\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I1209 12:28:45.383246   22636 provision.go:87] duration metric: took 18.4928918s to configureAuth
I1209 12:28:45.383246   22636 buildroot.go:189] setting minikube options for container-runtime
I1209 12:28:45.385137   22636 config.go:182] Loaded profile config "minikube": Driver=hyperv, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1209 12:28:45.385675   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:28:47.948586   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:28:47.948586   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:47.948586   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:28:51.262289   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:28:51.262289   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:51.265406   22636 main.go:141] libmachine: Using SSH client type: native
I1209 12:28:51.265969   22636 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8cc9c0] 0x8cf5a0 <nil>  [] 0s} 172.17.230.63 22 <nil> <nil>}
I1209 12:28:51.265969   22636 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1209 12:28:51.377800   22636 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I1209 12:28:51.377800   22636 buildroot.go:70] root file system type: tmpfs
I1209 12:28:51.378576   22636 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1209 12:28:51.378576   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:28:53.462943   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:28:53.462943   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:53.462943   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:28:56.274934   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:28:56.274934   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:56.278166   22636 main.go:141] libmachine: Using SSH client type: native
I1209 12:28:56.278166   22636 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8cc9c0] 0x8cf5a0 <nil>  [] 0s} 172.17.230.63 22 <nil> <nil>}
I1209 12:28:56.278166   22636 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperv --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1209 12:28:56.417695   22636 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperv --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1209 12:28:56.417695   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:28:59.099125   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:28:59.099125   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:28:59.099125   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:29:02.478862   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:29:02.478862   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:29:02.483822   22636 main.go:141] libmachine: Using SSH client type: native
I1209 12:29:02.484485   22636 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8cc9c0] 0x8cf5a0 <nil>  [] 0s} 172.17.230.63 22 <nil> <nil>}
I1209 12:29:02.484485   22636 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1209 12:29:04.353258   22636 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /usr/lib/systemd/system/docker.service.

I1209 12:29:04.353258   22636 machine.go:96] duration metric: took 55.3971314s to provisionDockerMachine
I1209 12:29:04.353258   22636 client.go:171] duration metric: took 2m1.6219771s to LocalClient.Create
I1209 12:29:04.353258   22636 start.go:167] duration metric: took 2m1.6219771s to libmachine.API.Create "minikube"
I1209 12:29:04.353258   22636 start.go:293] postStartSetup for "minikube" (driver="hyperv")
I1209 12:29:04.353258   22636 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1209 12:29:04.354841   22636 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1209 12:29:04.354841   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:29:06.533342   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:29:06.533342   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:29:06.533342   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:29:09.405578   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:29:09.405578   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:29:09.405578   22636 sshutil.go:53] new ssh client: &{IP:172.17.230.63 Port:22 SSHKeyPath:C:\Users\pusto\.minikube\machines\minikube\id_rsa Username:docker}
I1209 12:29:09.534481   22636 ssh_runner.go:235] Completed: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs: (5.1796408s)
I1209 12:29:09.547533   22636 ssh_runner.go:195] Run: cat /etc/os-release
I1209 12:29:09.554807   22636 info.go:137] Remote host: Buildroot 2023.02.9
I1209 12:29:09.555315   22636 filesync.go:126] Scanning C:\Users\pusto\.minikube\addons for local assets ...
I1209 12:29:09.555887   22636 filesync.go:126] Scanning C:\Users\pusto\.minikube\files for local assets ...
I1209 12:29:09.555887   22636 start.go:296] duration metric: took 5.2026295s for postStartSetup
I1209 12:29:09.559044   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:29:12.243009   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:29:12.243009   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:29:12.243009   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:29:15.596940   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:29:15.596940   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:29:15.596940   22636 profile.go:143] Saving config to C:\Users\pusto\.minikube\profiles\minikube\config.json ...
I1209 12:29:15.600222   22636 start.go:128] duration metric: took 2m12.8743487s to createHost
I1209 12:29:15.600222   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:29:17.901182   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:29:17.901182   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:29:17.901954   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:29:20.544042   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:29:20.544042   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:29:20.547575   22636 main.go:141] libmachine: Using SSH client type: native
I1209 12:29:20.548105   22636 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8cc9c0] 0x8cf5a0 <nil>  [] 0s} 172.17.230.63 22 <nil> <nil>}
I1209 12:29:20.548105   22636 main.go:141] libmachine: About to run SSH command:
date +%s.%N
I1209 12:29:20.665236   22636 main.go:141] libmachine: SSH cmd err, output: <nil>: 1733768883.887085688

I1209 12:29:20.665236   22636 fix.go:216] guest clock: 1733768883.887085688
I1209 12:29:20.665236   22636 fix.go:229] Guest: 2024-12-09 12:28:03.887085688 -0600 CST Remote: 2024-12-09 12:29:15.6002227 -0600 CST m=+247.572713201 (delta=-1m11.713137012s)
I1209 12:29:20.665236   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:29:23.222709   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:29:23.222709   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:29:23.222709   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:29:26.629037   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:29:26.629037   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:29:26.633364   22636 main.go:141] libmachine: Using SSH client type: native
I1209 12:29:26.634386   22636 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8cc9c0] 0x8cf5a0 <nil>  [] 0s} 172.17.230.63 22 <nil> <nil>}
I1209 12:29:26.634386   22636 main.go:141] libmachine: About to run SSH command:
sudo date -s @1733768960
I1209 12:29:26.779709   22636 main.go:141] libmachine: SSH cmd err, output: <nil>: Mon Dec  9 18:29:20 UTC 2024

I1209 12:29:26.779709   22636 fix.go:236] clock set: Mon Dec  9 18:29:20 UTC 2024
 (err=<nil>)
I1209 12:29:26.779709   22636 start.go:83] releasing machines lock for "minikube", held for 2m24.0538358s
I1209 12:29:26.779709   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:29:29.247552   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:29:29.247552   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:29:29.247552   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:29:31.887238   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:29:31.887238   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:29:31.890251   22636 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1209 12:29:31.890794   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:29:31.901216   22636 ssh_runner.go:195] Run: cat /version.json
I1209 12:29:31.901216   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:29:34.732108   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:29:34.732108   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:29:34.732108   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:29:34.753473   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:29:34.753473   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:29:34.753473   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:29:38.587195   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:29:38.587195   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:29:38.587195   22636 sshutil.go:53] new ssh client: &{IP:172.17.230.63 Port:22 SSHKeyPath:C:\Users\pusto\.minikube\machines\minikube\id_rsa Username:docker}
I1209 12:29:38.600018   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:29:38.600018   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:29:38.600018   22636 sshutil.go:53] new ssh client: &{IP:172.17.230.63 Port:22 SSHKeyPath:C:\Users\pusto\.minikube\machines\minikube\id_rsa Username:docker}
I1209 12:29:38.683536   22636 ssh_runner.go:235] Completed: curl.exe -sS -m 2 https://registry.k8s.io/: (6.793285s)
W1209 12:29:38.683536   22636 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1209 12:29:38.683536   22636 ssh_runner.go:235] Completed: cat /version.json: (6.7823204s)
I1209 12:29:38.696318   22636 ssh_runner.go:195] Run: systemctl --version
I1209 12:29:38.712859   22636 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W1209 12:29:38.720433   22636 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I1209 12:29:38.721776   22636 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1209 12:29:38.745687   22636 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1209 12:29:38.745687   22636 start.go:495] detecting cgroup driver to use...
I1209 12:29:38.746192   22636 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1209 12:29:38.775880   22636 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1209 12:29:38.797259   22636 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1209 12:29:38.808432   22636 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1209 12:29:38.815777   22636 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1209 12:29:38.839270   22636 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1209 12:29:38.866182   22636 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1209 12:29:38.890762   22636 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1209 12:29:38.915946   22636 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1209 12:29:38.944644   22636 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1209 12:29:38.973009   22636 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1209 12:29:38.999343   22636 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1209 12:29:39.019190   22636 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1209 12:29:39.038793   22636 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1209 12:29:39.055660   22636 ssh_runner.go:195] Run: sudo systemctl daemon-reload
W1209 12:29:39.059769   22636 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube VM
W1209 12:29:39.060277   22636 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1209 12:29:39.214176   22636 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1209 12:29:39.237831   22636 start.go:495] detecting cgroup driver to use...
I1209 12:29:39.239467   22636 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1209 12:29:39.258410   22636 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1209 12:29:39.276312   22636 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1209 12:29:39.299416   22636 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1209 12:29:39.320465   22636 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1209 12:29:39.342933   22636 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I1209 12:29:39.372663   22636 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1209 12:29:39.392190   22636 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1209 12:29:39.422391   22636 ssh_runner.go:195] Run: which cri-dockerd
I1209 12:29:39.427185   22636 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1209 12:29:39.437156   22636 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1209 12:29:39.464865   22636 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1209 12:29:39.619021   22636 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1209 12:29:39.766450   22636 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1209 12:29:39.766450   22636 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1209 12:29:39.793441   22636 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1209 12:29:39.964598   22636 ssh_runner.go:195] Run: sudo systemctl restart docker
I1209 12:29:42.305626   22636 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.3410281s)
I1209 12:29:42.306677   22636 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1209 12:29:42.322973   22636 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1209 12:29:42.337166   22636 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1209 12:29:42.445071   22636 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1209 12:29:42.557884   22636 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1209 12:29:42.661938   22636 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1209 12:29:42.679995   22636 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1209 12:29:42.693471   22636 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1209 12:29:42.795512   22636 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1209 12:29:42.858891   22636 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1209 12:29:42.867181   22636 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1209 12:29:42.872336   22636 start.go:563] Will wait 60s for crictl version
I1209 12:29:42.878661   22636 ssh_runner.go:195] Run: which crictl
I1209 12:29:42.883733   22636 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1209 12:29:42.915340   22636 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1209 12:29:42.919767   22636 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1209 12:29:42.942386   22636 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1209 12:29:42.967100   22636 out.go:235] 🐳  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1209 12:29:42.967100   22636 ip.go:176] getIPForInterface: searching for "vEthernet (Default Switch)"
I1209 12:29:42.973152   22636 ip.go:190] "Local Area Connection* 1" does not match prefix "vEthernet (Default Switch)"
I1209 12:29:42.973152   22636 ip.go:190] "Local Area Connection* 2" does not match prefix "vEthernet (Default Switch)"
I1209 12:29:42.973152   22636 ip.go:190] "Wi-Fi" does not match prefix "vEthernet (Default Switch)"
I1209 12:29:42.973152   22636 ip.go:190] "Bluetooth Network Connection" does not match prefix "vEthernet (Default Switch)"
I1209 12:29:42.973152   22636 ip.go:190] "Loopback Pseudo-Interface 1" does not match prefix "vEthernet (Default Switch)"
I1209 12:29:42.973152   22636 ip.go:185] found prefix matching interface for "vEthernet (Default Switch)": "vEthernet (Default Switch)"
I1209 12:29:42.973152   22636 ip.go:211] Found interface: {Index:25 MTU:1500 Name:vEthernet (Default Switch) HardwareAddr:00:15:5d:a9:38:d2 Flags:up|broadcast|multicast|running}
I1209 12:29:42.975874   22636 ip.go:214] interface addr: fe80::f6e2:acb:7cca:d5d0/64
I1209 12:29:42.975874   22636 ip.go:214] interface addr: 172.17.224.1/20
I1209 12:29:42.982546   22636 ssh_runner.go:195] Run: grep 172.17.224.1	host.minikube.internal$ /etc/hosts
I1209 12:29:42.986910   22636 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "172.17.224.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1209 12:29:43.004474   22636 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:6000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.17.230.63 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\pusto:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1209 12:29:43.004474   22636 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1209 12:29:43.008837   22636 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1209 12:29:43.023640   22636 docker.go:685] Got preloaded images: 
I1209 12:29:43.023640   22636 docker.go:691] registry.k8s.io/kube-apiserver:v1.31.0 wasn't preloaded
I1209 12:29:43.024882   22636 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I1209 12:29:43.041613   22636 ssh_runner.go:195] Run: which lz4
I1209 12:29:43.051838   22636 ssh_runner.go:195] Run: stat -c "%s %y" /preloaded.tar.lz4
I1209 12:29:43.056789   22636 ssh_runner.go:352] existence check for /preloaded.tar.lz4: stat -c "%s %y" /preloaded.tar.lz4: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/preloaded.tar.lz4': No such file or directory
I1209 12:29:43.056789   22636 ssh_runner.go:362] scp C:\Users\pusto\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 --> /preloaded.tar.lz4 (342554258 bytes)
I1209 12:29:44.470784   22636 docker.go:649] duration metric: took 1.4250875s to copy over tarball
I1209 12:29:44.472076   22636 ssh_runner.go:195] Run: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4
I1209 12:29:47.320278   22636 ssh_runner.go:235] Completed: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4: (2.8482024s)
I1209 12:29:47.320278   22636 ssh_runner.go:146] rm: /preloaded.tar.lz4
I1209 12:29:47.373488   22636 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I1209 12:29:47.385283   22636 ssh_runner.go:362] scp memory --> /var/lib/docker/image/overlay2/repositories.json (2631 bytes)
I1209 12:29:47.405586   22636 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1209 12:29:47.563388   22636 ssh_runner.go:195] Run: sudo systemctl restart docker
I1209 12:29:50.277160   22636 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.7137719s)
I1209 12:29:50.283146   22636 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1209 12:29:50.304721   22636 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1209 12:29:50.304721   22636 cache_images.go:84] Images are preloaded, skipping loading
I1209 12:29:50.304721   22636 kubeadm.go:934] updating node { 172.17.230.63 8443 v1.31.0 docker true true} ...
I1209 12:29:50.304721   22636 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=172.17.230.63

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1209 12:29:50.310594   22636 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1209 12:29:50.368578   22636 cni.go:84] Creating CNI manager for ""
I1209 12:29:50.368578   22636 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1209 12:29:50.368578   22636 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1209 12:29:50.368578   22636 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:172.17.230.63 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "172.17.230.63"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:172.17.230.63 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1209 12:29:50.368578   22636 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 172.17.230.63
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 172.17.230.63
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "172.17.230.63"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1209 12:29:50.370215   22636 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1209 12:29:50.382044   22636 binaries.go:44] Found k8s binaries, skipping transfer
I1209 12:29:50.383682   22636 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1209 12:29:50.399061   22636 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (308 bytes)
I1209 12:29:50.418463   22636 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1209 12:29:50.441835   22636 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2153 bytes)
I1209 12:29:50.472565   22636 ssh_runner.go:195] Run: grep 172.17.230.63	control-plane.minikube.internal$ /etc/hosts
I1209 12:29:50.477169   22636 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "172.17.230.63	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1209 12:29:50.493449   22636 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1209 12:29:50.674404   22636 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1209 12:29:50.698138   22636 certs.go:68] Setting up C:\Users\pusto\.minikube\profiles\minikube for IP: 172.17.230.63
I1209 12:29:50.698138   22636 certs.go:194] generating shared ca certs ...
I1209 12:29:50.698658   22636 certs.go:226] acquiring lock for ca certs: {Name:mk19e33d0c7eb1b2a75136568005d9307bb2a2fe Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 12:29:50.699230   22636 certs.go:240] generating "minikubeCA" ca cert: C:\Users\pusto\.minikube\ca.key
I1209 12:29:50.816526   22636 crypto.go:156] Writing cert to C:\Users\pusto\.minikube\ca.crt ...
I1209 12:29:50.817216   22636 lock.go:35] WriteFile acquiring C:\Users\pusto\.minikube\ca.crt: {Name:mk7f18a76e4232b3fe5283f3e1a8acc1660790c8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 12:29:50.817876   22636 crypto.go:164] Writing key to C:\Users\pusto\.minikube\ca.key ...
I1209 12:29:50.817876   22636 lock.go:35] WriteFile acquiring C:\Users\pusto\.minikube\ca.key: {Name:mke2e46f4b322cb10ca1c9e54c242300223a2550 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 12:29:50.819014   22636 certs.go:240] generating "proxyClientCA" ca cert: C:\Users\pusto\.minikube\proxy-client-ca.key
I1209 12:29:51.151074   22636 crypto.go:156] Writing cert to C:\Users\pusto\.minikube\proxy-client-ca.crt ...
I1209 12:29:51.151074   22636 lock.go:35] WriteFile acquiring C:\Users\pusto\.minikube\proxy-client-ca.crt: {Name:mk6f380520850ddc52cd160b3fa93586ac60e576 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 12:29:51.152082   22636 crypto.go:164] Writing key to C:\Users\pusto\.minikube\proxy-client-ca.key ...
I1209 12:29:51.152082   22636 lock.go:35] WriteFile acquiring C:\Users\pusto\.minikube\proxy-client-ca.key: {Name:mkbe509e41edd449c29fb0449f5e6f0b8007cb1f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 12:29:51.153083   22636 certs.go:256] generating profile certs ...
I1209 12:29:51.155113   22636 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\pusto\.minikube\profiles\minikube\client.key
I1209 12:29:51.155113   22636 crypto.go:68] Generating cert C:\Users\pusto\.minikube\profiles\minikube\client.crt with IP's: []
I1209 12:29:51.224715   22636 crypto.go:156] Writing cert to C:\Users\pusto\.minikube\profiles\minikube\client.crt ...
I1209 12:29:51.224715   22636 lock.go:35] WriteFile acquiring C:\Users\pusto\.minikube\profiles\minikube\client.crt: {Name:mke67e58fb3dcfcd738e7225fe59076490e62937 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 12:29:51.226727   22636 crypto.go:164] Writing key to C:\Users\pusto\.minikube\profiles\minikube\client.key ...
I1209 12:29:51.226727   22636 lock.go:35] WriteFile acquiring C:\Users\pusto\.minikube\profiles\minikube\client.key: {Name:mkf93badb6b71b7586e715cba0334e38a137a9c2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 12:29:51.227727   22636 certs.go:363] generating signed profile cert for "minikube": C:\Users\pusto\.minikube\profiles\minikube\apiserver.key.1b101624
I1209 12:29:51.227727   22636 crypto.go:68] Generating cert C:\Users\pusto\.minikube\profiles\minikube\apiserver.crt.1b101624 with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 172.17.230.63]
I1209 12:29:51.382246   22636 crypto.go:156] Writing cert to C:\Users\pusto\.minikube\profiles\minikube\apiserver.crt.1b101624 ...
I1209 12:29:51.382246   22636 lock.go:35] WriteFile acquiring C:\Users\pusto\.minikube\profiles\minikube\apiserver.crt.1b101624: {Name:mka49e48c33a41d09fd5529254b2ad5f3bc6f7ba Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 12:29:51.383897   22636 crypto.go:164] Writing key to C:\Users\pusto\.minikube\profiles\minikube\apiserver.key.1b101624 ...
I1209 12:29:51.383897   22636 lock.go:35] WriteFile acquiring C:\Users\pusto\.minikube\profiles\minikube\apiserver.key.1b101624: {Name:mk794bf1fe4efdb85735c9c8c321f7e70e7411ab Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 12:29:51.384422   22636 certs.go:381] copying C:\Users\pusto\.minikube\profiles\minikube\apiserver.crt.1b101624 -> C:\Users\pusto\.minikube\profiles\minikube\apiserver.crt
I1209 12:29:51.412025   22636 certs.go:385] copying C:\Users\pusto\.minikube\profiles\minikube\apiserver.key.1b101624 -> C:\Users\pusto\.minikube\profiles\minikube\apiserver.key
I1209 12:29:51.413209   22636 certs.go:363] generating signed profile cert for "aggregator": C:\Users\pusto\.minikube\profiles\minikube\proxy-client.key
I1209 12:29:51.413209   22636 crypto.go:68] Generating cert C:\Users\pusto\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I1209 12:29:51.623893   22636 crypto.go:156] Writing cert to C:\Users\pusto\.minikube\profiles\minikube\proxy-client.crt ...
I1209 12:29:51.623893   22636 lock.go:35] WriteFile acquiring C:\Users\pusto\.minikube\profiles\minikube\proxy-client.crt: {Name:mka7bb69f109392a12679b51afcffb712cb1e7b1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 12:29:51.624896   22636 crypto.go:164] Writing key to C:\Users\pusto\.minikube\profiles\minikube\proxy-client.key ...
I1209 12:29:51.624896   22636 lock.go:35] WriteFile acquiring C:\Users\pusto\.minikube\profiles\minikube\proxy-client.key: {Name:mkf07de8321f7f7bd5d84a659cb496d14e9a646e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 12:29:51.649486   22636 certs.go:484] found cert: C:\Users\pusto\.minikube\certs\ca-key.pem (1679 bytes)
I1209 12:29:51.650060   22636 certs.go:484] found cert: C:\Users\pusto\.minikube\certs\ca.pem (1074 bytes)
I1209 12:29:51.650060   22636 certs.go:484] found cert: C:\Users\pusto\.minikube\certs\cert.pem (1119 bytes)
I1209 12:29:51.650597   22636 certs.go:484] found cert: C:\Users\pusto\.minikube\certs\key.pem (1679 bytes)
I1209 12:29:51.652839   22636 ssh_runner.go:362] scp C:\Users\pusto\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1209 12:29:51.685863   22636 ssh_runner.go:362] scp C:\Users\pusto\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1209 12:29:51.720555   22636 ssh_runner.go:362] scp C:\Users\pusto\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1209 12:29:51.755651   22636 ssh_runner.go:362] scp C:\Users\pusto\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1209 12:29:51.794507   22636 ssh_runner.go:362] scp C:\Users\pusto\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1209 12:29:51.830437   22636 ssh_runner.go:362] scp C:\Users\pusto\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1209 12:29:51.859237   22636 ssh_runner.go:362] scp C:\Users\pusto\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1209 12:29:51.888032   22636 ssh_runner.go:362] scp C:\Users\pusto\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1209 12:29:51.921046   22636 ssh_runner.go:362] scp C:\Users\pusto\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1209 12:29:51.950055   22636 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1209 12:29:51.976607   22636 ssh_runner.go:195] Run: openssl version
I1209 12:29:51.986910   22636 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1209 12:29:52.058772   22636 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1209 12:29:52.066804   22636 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Dec  9  2024 /usr/share/ca-certificates/minikubeCA.pem
I1209 12:29:52.078714   22636 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1209 12:29:52.091875   22636 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1209 12:29:52.132674   22636 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1209 12:29:52.141131   22636 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1209 12:29:52.141131   22636 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:6000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.17.230.63 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\pusto:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1209 12:29:52.149720   22636 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1209 12:29:52.221671   22636 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1209 12:29:52.264346   22636 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1209 12:29:52.308674   22636 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1209 12:29:52.337650   22636 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1209 12:29:52.337650   22636 kubeadm.go:157] found existing configuration files:

I1209 12:29:52.339334   22636 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1209 12:29:52.350451   22636 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1209 12:29:52.352380   22636 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1209 12:29:52.364099   22636 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1209 12:29:52.375995   22636 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1209 12:29:52.378158   22636 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1209 12:29:52.390755   22636 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1209 12:29:52.405073   22636 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1209 12:29:52.406878   22636 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1209 12:29:52.427515   22636 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1209 12:29:52.441593   22636 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1209 12:29:52.443183   22636 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1209 12:29:52.455795   22636 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I1209 12:29:52.505298   22636 kubeadm.go:310] W1209 18:28:35.726694    1725 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I1209 12:29:52.506406   22636 kubeadm.go:310] W1209 18:28:35.727832    1725 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I1209 12:29:52.622909   22636 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1209 12:30:04.046851   22636 kubeadm.go:310] [init] Using Kubernetes version: v1.31.0
I1209 12:30:04.046851   22636 kubeadm.go:310] [preflight] Running pre-flight checks
I1209 12:30:04.046851   22636 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1209 12:30:04.047851   22636 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1209 12:30:04.047851   22636 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1209 12:30:04.047851   22636 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1209 12:30:04.050079   22636 out.go:235]     ▪ Generating certificates and keys ...
I1209 12:30:04.050622   22636 kubeadm.go:310] [certs] Using existing ca certificate authority
I1209 12:30:04.051178   22636 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1209 12:30:04.051178   22636 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I1209 12:30:04.051178   22636 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I1209 12:30:04.051730   22636 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I1209 12:30:04.051730   22636 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I1209 12:30:04.051730   22636 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I1209 12:30:04.051730   22636 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [172.17.230.63 127.0.0.1 ::1]
I1209 12:30:04.052275   22636 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I1209 12:30:04.052275   22636 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [172.17.230.63 127.0.0.1 ::1]
I1209 12:30:04.052807   22636 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I1209 12:30:04.052807   22636 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I1209 12:30:04.052807   22636 kubeadm.go:310] [certs] Generating "sa" key and public key
I1209 12:30:04.052807   22636 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1209 12:30:04.052807   22636 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1209 12:30:04.052807   22636 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1209 12:30:04.053353   22636 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1209 12:30:04.053353   22636 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1209 12:30:04.053353   22636 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1209 12:30:04.053895   22636 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1209 12:30:04.053895   22636 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1209 12:30:04.054984   22636 out.go:235]     ▪ Booting up control plane ...
I1209 12:30:04.055541   22636 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1209 12:30:04.055541   22636 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1209 12:30:04.056081   22636 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1209 12:30:04.056449   22636 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1209 12:30:04.056449   22636 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1209 12:30:04.056449   22636 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1209 12:30:04.056980   22636 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1209 12:30:04.056980   22636 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1209 12:30:04.057673   22636 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 502.795753ms
I1209 12:30:04.057673   22636 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I1209 12:30:04.057673   22636 kubeadm.go:310] [api-check] The API server is healthy after 6.503709632s
I1209 12:30:04.058216   22636 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1209 12:30:04.058216   22636 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1209 12:30:04.058216   22636 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I1209 12:30:04.058746   22636 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1209 12:30:04.058746   22636 kubeadm.go:310] [bootstrap-token] Using token: 8ayw7f.ckr6ycxyhxet6e93
I1209 12:30:04.059836   22636 out.go:235]     ▪ Configuring RBAC rules ...
I1209 12:30:04.059836   22636 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1209 12:30:04.060419   22636 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1209 12:30:04.060945   22636 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1209 12:30:04.060945   22636 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1209 12:30:04.060945   22636 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1209 12:30:04.061599   22636 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1209 12:30:04.061599   22636 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1209 12:30:04.061599   22636 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I1209 12:30:04.061599   22636 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I1209 12:30:04.061599   22636 kubeadm.go:310] 
I1209 12:30:04.062134   22636 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I1209 12:30:04.062134   22636 kubeadm.go:310] 
I1209 12:30:04.062134   22636 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I1209 12:30:04.062134   22636 kubeadm.go:310] 
I1209 12:30:04.062134   22636 kubeadm.go:310]   mkdir -p $HOME/.kube
I1209 12:30:04.062134   22636 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1209 12:30:04.062134   22636 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1209 12:30:04.062134   22636 kubeadm.go:310] 
I1209 12:30:04.062668   22636 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I1209 12:30:04.062668   22636 kubeadm.go:310] 
I1209 12:30:04.062668   22636 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1209 12:30:04.062668   22636 kubeadm.go:310] 
I1209 12:30:04.062668   22636 kubeadm.go:310] You should now deploy a pod network to the cluster.
I1209 12:30:04.063195   22636 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1209 12:30:04.063195   22636 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1209 12:30:04.063195   22636 kubeadm.go:310] 
I1209 12:30:04.063195   22636 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I1209 12:30:04.063195   22636 kubeadm.go:310] and service account keys on each node and then running the following as root:
I1209 12:30:04.063195   22636 kubeadm.go:310] 
I1209 12:30:04.063742   22636 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token 8ayw7f.ckr6ycxyhxet6e93 \
I1209 12:30:04.063742   22636 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:eb224d0087464c690addda1f0a1c312e05d30eb4b978ef993be7726e23ada941 \
I1209 12:30:04.063742   22636 kubeadm.go:310] 	--control-plane 
I1209 12:30:04.063742   22636 kubeadm.go:310] 
I1209 12:30:04.063742   22636 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I1209 12:30:04.063742   22636 kubeadm.go:310] 
I1209 12:30:04.063742   22636 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token 8ayw7f.ckr6ycxyhxet6e93 \
I1209 12:30:04.064285   22636 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:eb224d0087464c690addda1f0a1c312e05d30eb4b978ef993be7726e23ada941 
I1209 12:30:04.064285   22636 cni.go:84] Creating CNI manager for ""
I1209 12:30:04.064285   22636 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1209 12:30:04.065398   22636 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I1209 12:30:04.068851   22636 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1209 12:30:04.082483   22636 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I1209 12:30:04.112120   22636 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1209 12:30:04.113918   22636 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1209 12:30:04.115094   22636 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_12_09T12_30_04_0700 minikube.k8s.io/version=v1.34.0 minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I1209 12:30:04.137054   22636 ops.go:34] apiserver oom_adj: -16
I1209 12:30:04.231043   22636 kubeadm.go:1113] duration metric: took 118.9237ms to wait for elevateKubeSystemPrivileges
I1209 12:30:04.271801   22636 kubeadm.go:394] duration metric: took 12.1306695s to StartCluster
I1209 12:30:04.271801   22636 settings.go:142] acquiring lock: {Name:mka5af6b3b551d6971d0b038b4f7f6a57c9fdede Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 12:30:04.271801   22636 settings.go:150] Updating kubeconfig:  C:\Users\pusto\.kube\config
I1209 12:30:04.274667   22636 lock.go:35] WriteFile acquiring C:\Users\pusto\.kube\config: {Name:mkaea95c34ee1f020e7d71372b1c344f15c66757 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 12:30:04.275756   22636 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1209 12:30:04.275756   22636 start.go:235] Will wait 6m0s for node &{Name: IP:172.17.230.63 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1209 12:30:04.276313   22636 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1209 12:30:04.276313   22636 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1209 12:30:04.276313   22636 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I1209 12:30:04.276313   22636 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1209 12:30:04.276313   22636 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1209 12:30:04.276313   22636 host.go:66] Checking if "minikube" exists ...
I1209 12:30:04.276900   22636 config.go:182] Loaded profile config "minikube": Driver=hyperv, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1209 12:30:04.277254   22636 out.go:177] 🔎  Verifying Kubernetes components...
I1209 12:30:04.278327   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:30:04.279320   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:30:04.283266   22636 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1209 12:30:04.628498   22636 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           172.17.224.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1209 12:30:04.896783   22636 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1209 12:30:05.522625   22636 start.go:971] {"host.minikube.internal": 172.17.224.1} host record injected into CoreDNS's ConfigMap
I1209 12:30:05.527539   22636 api_server.go:52] waiting for apiserver process to appear ...
I1209 12:30:05.530145   22636 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 12:30:05.561328   22636 api_server.go:72] duration metric: took 1.2855714s to wait for apiserver process to appear ...
I1209 12:30:05.561328   22636 api_server.go:88] waiting for apiserver healthz status ...
I1209 12:30:05.561328   22636 api_server.go:253] Checking apiserver healthz at https://172.17.230.63:8443/healthz ...
I1209 12:30:05.577315   22636 api_server.go:279] https://172.17.230.63:8443/healthz returned 200:
ok
I1209 12:30:05.580144   22636 api_server.go:141] control plane version: v1.31.0
I1209 12:30:05.580144   22636 api_server.go:131] duration metric: took 18.8161ms to wait for apiserver health ...
I1209 12:30:05.580144   22636 system_pods.go:43] waiting for kube-system pods to appear ...
I1209 12:30:05.603581   22636 system_pods.go:59] 4 kube-system pods found
I1209 12:30:05.603581   22636 system_pods.go:61] "etcd-minikube" [3929a372-4d5e-4b79-8461-fef0dc6690d1] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1209 12:30:05.603581   22636 system_pods.go:61] "kube-apiserver-minikube" [fd3b5644-c8ad-401a-a82f-30b80ef2469d] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1209 12:30:05.603581   22636 system_pods.go:61] "kube-controller-manager-minikube" [3382742d-1d6a-4b79-9f9c-353e9390d7ae] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1209 12:30:05.603581   22636 system_pods.go:61] "kube-scheduler-minikube" [7662a99c-d113-4d8c-8ec5-50af5ed441e5] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1209 12:30:05.603581   22636 system_pods.go:74] duration metric: took 23.4372ms to wait for pod list to return data ...
I1209 12:30:05.603581   22636 kubeadm.go:582] duration metric: took 1.3278247s to wait for: map[apiserver:true system_pods:true]
I1209 12:30:05.603581   22636 node_conditions.go:102] verifying NodePressure condition ...
I1209 12:30:05.610763   22636 node_conditions.go:122] node storage ephemeral capacity is 17734596Ki
I1209 12:30:05.610763   22636 node_conditions.go:123] node cpu capacity is 2
I1209 12:30:05.610763   22636 node_conditions.go:105] duration metric: took 7.1825ms to run NodePressure ...
I1209 12:30:05.610763   22636 start.go:241] waiting for startup goroutines ...
I1209 12:30:06.052604   22636 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1209 12:30:07.715081   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:30:07.715081   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:30:07.716402   22636 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1209 12:30:07.718021   22636 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1209 12:30:07.718021   22636 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1209 12:30:07.718021   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:30:07.754961   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:30:07.754961   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:30:07.757678   22636 addons.go:234] Setting addon default-storageclass=true in "minikube"
I1209 12:30:07.757678   22636 host.go:66] Checking if "minikube" exists ...
I1209 12:30:07.758765   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:30:10.734591   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:30:10.734591   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:30:10.734591   22636 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1209 12:30:10.734591   22636 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1209 12:30:10.734591   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1209 12:30:10.742370   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:30:10.742370   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:30:10.742370   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:30:13.852295   22636 main.go:141] libmachine: [stdout =====>] : Running

I1209 12:30:13.852295   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:30:13.852295   22636 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1209 12:30:14.484122   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:30:14.484122   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:30:14.484571   22636 sshutil.go:53] new ssh client: &{IP:172.17.230.63 Port:22 SSHKeyPath:C:\Users\pusto\.minikube\machines\minikube\id_rsa Username:docker}
I1209 12:30:14.600003   22636 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1209 12:30:17.363864   22636 main.go:141] libmachine: [stdout =====>] : 172.17.230.63

I1209 12:30:17.363864   22636 main.go:141] libmachine: [stderr =====>] : 
I1209 12:30:17.363864   22636 sshutil.go:53] new ssh client: &{IP:172.17.230.63 Port:22 SSHKeyPath:C:\Users\pusto\.minikube\machines\minikube\id_rsa Username:docker}
I1209 12:30:17.465557   22636 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1209 12:30:17.641998   22636 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I1209 12:30:17.643617   22636 addons.go:510] duration metric: took 13.3673038s for enable addons: enabled=[storage-provisioner default-storageclass]
I1209 12:30:17.643617   22636 start.go:246] waiting for cluster config update ...
I1209 12:30:17.643617   22636 start.go:255] writing updated cluster config ...
I1209 12:30:17.658049   22636 ssh_runner.go:195] Run: rm -f paused
I1209 12:30:17.816004   22636 start.go:600] kubectl: 1.30.5, cluster: 1.31.0 (minor skew: 1)
I1209 12:30:17.819948   22636 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Dec 09 18:30:20 minikube dockerd[1412]: time="2024-12-09T18:30:20.050068149Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 09 18:30:20 minikube dockerd[1412]: time="2024-12-09T18:30:20.050150548Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 09 18:30:20 minikube dockerd[1412]: time="2024-12-09T18:30:20.063606278Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Dec 09 18:30:20 minikube dockerd[1412]: time="2024-12-09T18:30:20.063767444Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 09 18:30:20 minikube dockerd[1412]: time="2024-12-09T18:30:20.063794784Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 09 18:30:20 minikube dockerd[1412]: time="2024-12-09T18:30:20.063855952Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 09 18:30:20 minikube cri-dockerd[1309]: time="2024-12-09T18:30:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4e999e10ad25fae845d75e6f8ad55f9220bccd39ad180c051753746c85d7669c/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 09 18:30:20 minikube cri-dockerd[1309]: time="2024-12-09T18:30:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4d17c6af23af9c4b908989b72b4158882740bdd0318decfcfe1fadf39bb118b4/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 09 18:30:20 minikube dockerd[1406]: time="2024-12-09T18:30:20.376511900Z" level=warning msg="reference for unknown type: " digest="sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3"
Dec 09 18:30:27 minikube cri-dockerd[1309]: time="2024-12-09T18:30:27Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3"
Dec 09 18:30:27 minikube cri-dockerd[1309]: time="2024-12-09T18:30:27Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3"
Dec 09 18:30:27 minikube dockerd[1412]: time="2024-12-09T18:30:27.476633915Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Dec 09 18:30:27 minikube dockerd[1412]: time="2024-12-09T18:30:27.476724011Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 09 18:30:27 minikube dockerd[1412]: time="2024-12-09T18:30:27.476740267Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 09 18:30:27 minikube dockerd[1412]: time="2024-12-09T18:30:27.476816635Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 09 18:30:27 minikube dockerd[1412]: time="2024-12-09T18:30:27.530736584Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Dec 09 18:30:27 minikube dockerd[1412]: time="2024-12-09T18:30:27.531346884Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 09 18:30:27 minikube dockerd[1412]: time="2024-12-09T18:30:27.531418612Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 09 18:30:27 minikube dockerd[1412]: time="2024-12-09T18:30:27.531636380Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 09 18:30:27 minikube dockerd[1412]: time="2024-12-09T18:30:27.535856458Z" level=info msg="shim disconnected" id=f06407d3d7bab4fc3d6dfb70d734862a60e5407d0b451f701d2e61993acf1aeb namespace=moby
Dec 09 18:30:27 minikube dockerd[1412]: time="2024-12-09T18:30:27.535973928Z" level=warning msg="cleaning up after shim disconnected" id=f06407d3d7bab4fc3d6dfb70d734862a60e5407d0b451f701d2e61993acf1aeb namespace=moby
Dec 09 18:30:27 minikube dockerd[1412]: time="2024-12-09T18:30:27.536001681Z" level=info msg="cleaning up dead shim" namespace=moby
Dec 09 18:30:27 minikube dockerd[1406]: time="2024-12-09T18:30:27.536015566Z" level=info msg="ignoring event" container=f06407d3d7bab4fc3d6dfb70d734862a60e5407d0b451f701d2e61993acf1aeb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 09 18:30:27 minikube dockerd[1406]: time="2024-12-09T18:30:27.593032826Z" level=info msg="ignoring event" container=8cf871d20b56cae6f60bf949d7f64c4b2b8ec7f43a001ed1ee4a52213b7debf5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 09 18:30:27 minikube dockerd[1412]: time="2024-12-09T18:30:27.592957606Z" level=info msg="shim disconnected" id=8cf871d20b56cae6f60bf949d7f64c4b2b8ec7f43a001ed1ee4a52213b7debf5 namespace=moby
Dec 09 18:30:27 minikube dockerd[1412]: time="2024-12-09T18:30:27.593426760Z" level=warning msg="cleaning up after shim disconnected" id=8cf871d20b56cae6f60bf949d7f64c4b2b8ec7f43a001ed1ee4a52213b7debf5 namespace=moby
Dec 09 18:30:27 minikube dockerd[1412]: time="2024-12-09T18:30:27.593440294Z" level=info msg="cleaning up dead shim" namespace=moby
Dec 09 18:30:28 minikube dockerd[1406]: time="2024-12-09T18:30:28.701593445Z" level=info msg="ignoring event" container=4d17c6af23af9c4b908989b72b4158882740bdd0318decfcfe1fadf39bb118b4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 09 18:30:28 minikube dockerd[1412]: time="2024-12-09T18:30:28.701665272Z" level=info msg="shim disconnected" id=4d17c6af23af9c4b908989b72b4158882740bdd0318decfcfe1fadf39bb118b4 namespace=moby
Dec 09 18:30:28 minikube dockerd[1412]: time="2024-12-09T18:30:28.701810231Z" level=warning msg="cleaning up after shim disconnected" id=4d17c6af23af9c4b908989b72b4158882740bdd0318decfcfe1fadf39bb118b4 namespace=moby
Dec 09 18:30:28 minikube dockerd[1412]: time="2024-12-09T18:30:28.701816102Z" level=info msg="cleaning up dead shim" namespace=moby
Dec 09 18:30:28 minikube dockerd[1412]: time="2024-12-09T18:30:28.703775464Z" level=info msg="shim disconnected" id=4e999e10ad25fae845d75e6f8ad55f9220bccd39ad180c051753746c85d7669c namespace=moby
Dec 09 18:30:28 minikube dockerd[1412]: time="2024-12-09T18:30:28.703801712Z" level=warning msg="cleaning up after shim disconnected" id=4e999e10ad25fae845d75e6f8ad55f9220bccd39ad180c051753746c85d7669c namespace=moby
Dec 09 18:30:28 minikube dockerd[1412]: time="2024-12-09T18:30:28.703806658Z" level=info msg="cleaning up dead shim" namespace=moby
Dec 09 18:30:28 minikube dockerd[1406]: time="2024-12-09T18:30:28.703920927Z" level=info msg="ignoring event" container=4e999e10ad25fae845d75e6f8ad55f9220bccd39ad180c051753746c85d7669c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 09 18:30:35 minikube dockerd[1412]: time="2024-12-09T18:30:35.638320456Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Dec 09 18:30:35 minikube dockerd[1412]: time="2024-12-09T18:30:35.638375223Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 09 18:30:35 minikube dockerd[1412]: time="2024-12-09T18:30:35.638573170Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 09 18:30:35 minikube dockerd[1412]: time="2024-12-09T18:30:35.638673355Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 09 18:30:35 minikube cri-dockerd[1309]: time="2024-12-09T18:30:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/37469be5e0a547397b32827ada7241a7d75105444bdf1ff140a5786344ea3eb0/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 09 18:30:35 minikube dockerd[1406]: time="2024-12-09T18:30:35.824375683Z" level=warning msg="reference for unknown type: " digest="sha256:d5f8217feeac4887cb1ed21f27c2674e58be06bd8f5184cacea2a69abaf78dce" remote="registry.k8s.io/ingress-nginx/controller@sha256:d5f8217feeac4887cb1ed21f27c2674e58be06bd8f5184cacea2a69abaf78dce"
Dec 09 18:30:47 minikube cri-dockerd[1309]: time="2024-12-09T18:30:47Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.2@sha256:d5f8217feeac4887cb1ed21f27c2674e58be06bd8f5184cacea2a69abaf78dce: e51022658205: Downloading [============================>                      ]  7.797MB/13.67MB"
Dec 09 18:30:57 minikube cri-dockerd[1309]: time="2024-12-09T18:30:57Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.2@sha256:d5f8217feeac4887cb1ed21f27c2674e58be06bd8f5184cacea2a69abaf78dce: d247ef832517: Downloading [==============================================>    ]  32.47MB/34.91MB"
Dec 09 18:31:01 minikube cri-dockerd[1309]: time="2024-12-09T18:31:01Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/controller:v1.11.2@sha256:d5f8217feeac4887cb1ed21f27c2674e58be06bd8f5184cacea2a69abaf78dce: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/controller@sha256:d5f8217feeac4887cb1ed21f27c2674e58be06bd8f5184cacea2a69abaf78dce"
Dec 09 18:31:01 minikube dockerd[1412]: time="2024-12-09T18:31:01.509329400Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Dec 09 18:31:01 minikube dockerd[1412]: time="2024-12-09T18:31:01.509376665Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 09 18:31:01 minikube dockerd[1412]: time="2024-12-09T18:31:01.509386680Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 09 18:31:01 minikube dockerd[1412]: time="2024-12-09T18:31:01.509437832Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 09 18:31:32 minikube dockerd[1412]: time="2024-12-09T18:31:32.916139083Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Dec 09 18:31:32 minikube dockerd[1412]: time="2024-12-09T18:31:32.916250395Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 09 18:31:32 minikube dockerd[1412]: time="2024-12-09T18:31:32.916264116Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 09 18:31:32 minikube dockerd[1412]: time="2024-12-09T18:31:32.916318494Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 09 18:31:32 minikube cri-dockerd[1309]: time="2024-12-09T18:31:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dd2f2211fd21c2c62537fd9a8ee6be0f9609158b9a13ddf2a50b7eccefec776e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 09 18:31:44 minikube cri-dockerd[1309]: time="2024-12-09T18:31:44Z" level=info msg="Pulling image imalouhov/kubernetes-homework:0.0.1: 96a8e5c4d318: Downloading [=========>                                         ]  36.08MB/187.9MB"
Dec 09 18:31:54 minikube cri-dockerd[1309]: time="2024-12-09T18:31:54Z" level=info msg="Pulling image imalouhov/kubernetes-homework:0.0.1: 96a8e5c4d318: Downloading [===================================>               ]  131.8MB/187.9MB"
Dec 09 18:32:03 minikube cri-dockerd[1309]: time="2024-12-09T18:32:03Z" level=info msg="Stop pulling image imalouhov/kubernetes-homework:0.0.1: Status: Downloaded newer image for imalouhov/kubernetes-homework:0.0.1"
Dec 09 18:32:04 minikube dockerd[1412]: time="2024-12-09T18:32:04.116453599Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Dec 09 18:32:04 minikube dockerd[1412]: time="2024-12-09T18:32:04.116491786Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 09 18:32:04 minikube dockerd[1412]: time="2024-12-09T18:32:04.116501272Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 09 18:32:04 minikube dockerd[1412]: time="2024-12-09T18:32:04.116546737Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
ab9068e848a2f       imalouhov/kubernetes-homework@sha256:733c25df0c48350be0f0c70be79c98e4c33c58352fe812344370eae2769c76b7                        20 minutes ago      Running             kubernetes-homework       0                   dd2f2211fd21c       kubernetes-homework-7668f8bd44-m9jgr
854484eca7b9c       registry.k8s.io/ingress-nginx/controller@sha256:d5f8217feeac4887cb1ed21f27c2674e58be06bd8f5184cacea2a69abaf78dce             21 minutes ago      Running             controller                0                   37469be5e0a54       ingress-nginx-controller-bc57996ff-98m2c
8cf871d20b56c       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3   21 minutes ago      Exited              patch                     0                   4d17c6af23af9       ingress-nginx-admission-patch-6jcdz
f06407d3d7bab       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3   21 minutes ago      Exited              create                    0                   4e999e10ad25f       ingress-nginx-admission-create-59fb2
7a1f24d56a3d5       6e38f40d628db                                                                                                                23 minutes ago      Running             storage-provisioner       0                   f181c17495168       storage-provisioner
1d3f422aea5e1       cbb01a7bd410d                                                                                                                23 minutes ago      Running             coredns                   0                   37a6770eccb3f       coredns-6f6b679f8f-8mxmd
a3addd9728d4c       ad83b2ca7b09e                                                                                                                23 minutes ago      Running             kube-proxy                0                   bb8278cfe2691       kube-proxy-w85qh
41bcb59bf5223       1766f54c897f0                                                                                                                23 minutes ago      Running             kube-scheduler            0                   48b02462c1fa8       kube-scheduler-minikube
e3e32081aa4ca       045733566833c                                                                                                                23 minutes ago      Running             kube-controller-manager   0                   0211f3550c9f8       kube-controller-manager-minikube
8a9967351e387       604f5db92eaa8                                                                                                                23 minutes ago      Running             kube-apiserver            0                   f694abc44a9fc       kube-apiserver-minikube
2fc7cab850062       2e96e5913fc06                                                                                                                23 minutes ago      Running             etcd                      0                   5ce117dd89673       etcd-minikube


==> controller_ingress [854484eca7b9] <==
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.11.2
  Build:         46e76e5916813cfca2a9b0bfdc34b69a0000f6b9
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.25.5

-------------------------------------------------------------------------------

W1209 18:31:01.546975       7 client_config.go:659] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I1209 18:31:01.547046       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
I1209 18:31:01.550107       7 main.go:248] "Running in Kubernetes cluster" major="1" minor="31" git="v1.31.0" state="clean" commit="9edcffcde5595e8a5b1a35f88c421764e575afce" platform="linux/amd64"
I1209 18:31:01.591037       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I1209 18:31:01.599568       7 ssl.go:535] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I1209 18:31:01.605314       7 nginx.go:271] "Starting NGINX Ingress controller"
I1209 18:31:01.609171       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"fc8cc884-d1fe-4a75-acd1-94fc23f54eaa", APIVersion:"v1", ResourceVersion:"487", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I1209 18:31:01.613612       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"728b885a-9459-41f1-94cc-ee6880a19f5c", APIVersion:"v1", ResourceVersion:"488", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I1209 18:31:01.613717       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"5a403d2e-7bf4-4349-9813-74a7b1189117", APIVersion:"v1", ResourceVersion:"489", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I1209 18:31:02.807410       7 nginx.go:317] "Starting NGINX process"
I1209 18:31:02.807726       7 leaderelection.go:250] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I1209 18:31:02.808427       7 nginx.go:337] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I1209 18:31:02.809520       7 controller.go:193] "Configuration changes detected, backend reload required"
I1209 18:31:02.815535       7 leaderelection.go:260] successfully acquired lease ingress-nginx/ingress-nginx-leader
I1209 18:31:02.815683       7 status.go:85] "New leader elected" identity="ingress-nginx-controller-bc57996ff-98m2c"
I1209 18:31:02.821175       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-bc57996ff-98m2c" node="minikube"
I1209 18:31:02.831018       7 controller.go:213] "Backend successfully reloaded"
I1209 18:31:02.831096       7 controller.go:224] "Initial sync, sleeping for 1 second"
I1209 18:31:02.831140       7 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-bc57996ff-98m2c", UID:"8aa2f6bd-43b7-46fe-a905-5149fa443ed4", APIVersion:"v1", ResourceVersion:"519", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
W1209 18:35:11.630666       7 controller.go:1216] Service "default/kubernetes-service" does not have any active Endpoint.
I1209 18:35:11.642457       7 admission.go:149] processed ingress via admission controller {testedIngressLength:1 testedIngressTime:0.012s renderingIngressLength:1 renderingIngressTime:0s admissionTime:0.012s testedConfigurationSize:21.9kB}
I1209 18:35:11.642501       7 main.go:107] "successfully validated configuration, accepting" ingress="default/kubernetes-ingress"
I1209 18:35:11.649629       7 store.go:440] "Found valid IngressClass" ingress="default/kubernetes-ingress" ingressclass="nginx"
I1209 18:35:11.650111       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"kubernetes-ingress", UID:"8206b85b-c679-45a1-bc71-c43c2d1a509a", APIVersion:"networking.k8s.io/v1", ResourceVersion:"872", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
W1209 18:35:11.650116       7 controller.go:1216] Service "default/kubernetes-service" does not have any active Endpoint.
I1209 18:35:11.650553       7 controller.go:193] "Configuration changes detected, backend reload required"
I1209 18:35:11.675164       7 controller.go:213] "Backend successfully reloaded"
I1209 18:35:11.675637       7 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-bc57996ff-98m2c", UID:"8aa2f6bd-43b7-46fe-a905-5149fa443ed4", APIVersion:"v1", ResourceVersion:"519", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I1209 18:36:02.825116       7 status.go:304] "updating Ingress status" namespace="default" ingress="kubernetes-ingress" currentValue=null newValue=[{"ip":"172.17.230.63"}]
W1209 18:36:02.832596       7 controller.go:1216] Service "default/kubernetes-service" does not have any active Endpoint.
I1209 18:36:02.833258       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"kubernetes-ingress", UID:"8206b85b-c679-45a1-bc71-c43c2d1a509a", APIVersion:"networking.k8s.io/v1", ResourceVersion:"921", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync


==> coredns [1d3f422aea5e] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 4be743d1b56fa0afc7ece7e3eaec761af420ec20bc31b53ffe2e37821d5f2dbf30ad4d3b94f94c28b795b70cbe328181f6f3d0b0c3889491886196bd2ae686b9
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:51475 - 57286 "HINFO IN 1665003148063246453.615945423265865037. udp 56 false 512" NXDOMAIN qr,rd,ra 131 0.035823644s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[105896546]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (09-Dec-2024 18:28:53.467) (total time: 30001ms):
Trace[105896546]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (18:29:23.468)
Trace[105896546]: [30.001790038s] [30.001790038s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[908675560]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (09-Dec-2024 18:28:53.468) (total time: 30000ms):
Trace[908675560]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (18:29:23.469)
Trace[908675560]: [30.00068481s] [30.00068481s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1852059711]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (09-Dec-2024 18:28:53.468) (total time: 30000ms):
Trace[1852059711]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (18:29:23.469)
Trace[1852059711]: [30.000952161s] [30.000952161s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_12_09T12_30_04_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 09 Dec 2024 18:28:43 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 09 Dec 2024 18:52:13 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 09 Dec 2024 18:47:36 +0000   Mon, 09 Dec 2024 18:28:40 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 09 Dec 2024 18:47:36 +0000   Mon, 09 Dec 2024 18:28:40 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 09 Dec 2024 18:47:36 +0000   Mon, 09 Dec 2024 18:28:40 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 09 Dec 2024 18:47:36 +0000   Mon, 09 Dec 2024 18:28:51 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  172.17.230.63
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             5924004Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             5924004Ki
  pods:               110
System Info:
  Machine ID:                 0ec231483d9944f79b324bbf5537f177
  System UUID:                d5809dba-6835-9444-8f95-f05b03718f24
  Boot ID:                    64282185-446c-4ee3-a62d-65d5c59bc6be
  Kernel Version:             5.10.207
  OS Image:                   Buildroot 2023.02.9
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  default                     kubernetes-homework-7668f8bd44-m9jgr        0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m
  ingress-nginx               ingress-nginx-controller-bc57996ff-98m2c    100m (5%)     0 (0%)      90Mi (1%)        0 (0%)         21m
  kube-system                 coredns-6f6b679f8f-8mxmd                    100m (5%)     0 (0%)      70Mi (1%)        170Mi (2%)     23m
  kube-system                 etcd-minikube                               100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         23m
  kube-system                 kube-apiserver-minikube                     250m (12%)    0 (0%)      0 (0%)           0 (0%)         23m
  kube-system                 kube-controller-manager-minikube            200m (10%)    0 (0%)      0 (0%)           0 (0%)         23m
  kube-system                 kube-proxy-w85qh                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         23m
  kube-system                 kube-scheduler-minikube                     100m (5%)     0 (0%)      0 (0%)           0 (0%)         23m
  kube-system                 storage-provisioner                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         23m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (42%)  0 (0%)
  memory             260Mi (4%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 23m                kube-proxy       
  Normal  NodeHasSufficientMemory  23m (x8 over 23m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    23m (x8 over 23m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     23m (x7 over 23m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  23m                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 23m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  23m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  23m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    23m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     23m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           23m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  NodeReady                23m                kubelet          Node minikube status is now: NodeReady


==> dmesg <==
[Dec 9 18:26] You have booted with nomodeset. This means your GPU drivers are DISABLED
[  +0.000001] Any video related functionality will be severely degraded, and you may not even be able to suspend the system properly
[  +0.000001] Unless you actually understand what nomodeset does, you should reboot without enabling it
[  +0.092858] Spectre V2 : WARNING: Unprivileged eBPF is enabled with eIBRS on, data leaks possible via Spectre v2 BHB attacks!
[  +0.074075] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended PCI configuration space under this bridge.
[  +0.010617] * Found PM-Timer Bug on the chipset. Due to workarounds for a bug,
              * this clock source is slow. Consider trying other clock sources
[  +6.410197] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2
[  +0.509391] psmouse serio1: trackpoint: failed to get extended button data, assuming 3 buttons
[  +1.222717] systemd-fstab-generator[115]: Ignoring "noauto" option for root device
[  +3.140146] NFSD: Using /var/lib/nfs/v4recovery as the NFSv4 state recovery directory
[  +0.000007] NFSD: unable to find recovery directory /var/lib/nfs/v4recovery
[  +0.000002] NFSD: Unable to initialize client recovery tracking! (-2)
[Dec 9 18:27] systemd-fstab-generator[635]: Ignoring "noauto" option for root device
[  +0.141826] systemd-fstab-generator[647]: Ignoring "noauto" option for root device
[Dec 9 18:28] systemd-fstab-generator[987]: Ignoring "noauto" option for root device
[  +0.073985] kauditd_printk_skb: 65 callbacks suppressed
[  +0.328655] systemd-fstab-generator[1023]: Ignoring "noauto" option for root device
[  +0.139978] systemd-fstab-generator[1035]: Ignoring "noauto" option for root device
[  +0.188366] systemd-fstab-generator[1049]: Ignoring "noauto" option for root device
[  +2.520785] systemd-fstab-generator[1262]: Ignoring "noauto" option for root device
[  +0.117193] systemd-fstab-generator[1274]: Ignoring "noauto" option for root device
[  +0.106288] systemd-fstab-generator[1286]: Ignoring "noauto" option for root device
[  +0.129915] systemd-fstab-generator[1301]: Ignoring "noauto" option for root device
[  +4.731669] systemd-fstab-generator[1398]: Ignoring "noauto" option for root device
[  +0.081374] kauditd_printk_skb: 202 callbacks suppressed
[  +3.008989] systemd-fstab-generator[1649]: Ignoring "noauto" option for root device
[  +4.995199] systemd-fstab-generator[1773]: Ignoring "noauto" option for root device
[  +0.057850] kauditd_printk_skb: 70 callbacks suppressed
[  +7.577722] systemd-fstab-generator[2173]: Ignoring "noauto" option for root device
[  +0.125330] kauditd_printk_skb: 62 callbacks suppressed
[  +1.222820] systemd-fstab-generator[2231]: Ignoring "noauto" option for root device
[  +4.939211] kauditd_printk_skb: 34 callbacks suppressed
[  +6.200923] kauditd_printk_skb: 47 callbacks suppressed
[Dec 9 18:30] kauditd_printk_skb: 20 callbacks suppressed
[  +8.160631] kauditd_printk_skb: 16 callbacks suppressed
[Dec 9 18:31] kauditd_printk_skb: 18 callbacks suppressed
[Dec 9 18:32] kauditd_printk_skb: 2 callbacks suppressed


==> etcd [2fc7cab85006] <==
{"level":"info","ts":"2024-12-09T18:28:40.318646Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"8f28a26edd0f883b","cluster-id":"6057d4e182cc4f48"}
{"level":"info","ts":"2024-12-09T18:28:40.318745Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8f28a26edd0f883b switched to configuration voters=()"}
{"level":"info","ts":"2024-12-09T18:28:40.318787Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8f28a26edd0f883b became follower at term 0"}
{"level":"info","ts":"2024-12-09T18:28:40.318798Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft 8f28a26edd0f883b [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-12-09T18:28:40.318807Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8f28a26edd0f883b became follower at term 1"}
{"level":"info","ts":"2024-12-09T18:28:40.318877Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8f28a26edd0f883b switched to configuration voters=(10315673543531006011)"}
{"level":"warn","ts":"2024-12-09T18:28:40.329233Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-12-09T18:28:40.339105Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-12-09T18:28:40.346121Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-12-09T18:28:40.355924Z","caller":"etcdserver/server.go:867","msg":"starting etcd server","local-member-id":"8f28a26edd0f883b","local-server-version":"3.5.15","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-12-09T18:28:40.361672Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-12-09T18:28:40.382932Z","caller":"etcdserver/server.go:751","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"8f28a26edd0f883b","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-12-09T18:28:40.383807Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-12-09T18:28:40.383855Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-12-09T18:28:40.383872Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-12-09T18:28:40.385419Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-12-09T18:28:40.386130Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"8f28a26edd0f883b","initial-advertise-peer-urls":["https://172.17.230.63:2380"],"listen-peer-urls":["https://172.17.230.63:2380"],"advertise-client-urls":["https://172.17.230.63:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://172.17.230.63:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-12-09T18:28:40.386210Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-12-09T18:28:40.386298Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"172.17.230.63:2380"}
{"level":"info","ts":"2024-12-09T18:28:40.386313Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"172.17.230.63:2380"}
{"level":"info","ts":"2024-12-09T18:28:40.388007Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8f28a26edd0f883b switched to configuration voters=(10315673543531006011)"}
{"level":"info","ts":"2024-12-09T18:28:40.390036Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"6057d4e182cc4f48","local-member-id":"8f28a26edd0f883b","added-peer-id":"8f28a26edd0f883b","added-peer-peer-urls":["https://172.17.230.63:2380"]}
{"level":"info","ts":"2024-12-09T18:28:40.919140Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8f28a26edd0f883b is starting a new election at term 1"}
{"level":"info","ts":"2024-12-09T18:28:40.919425Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8f28a26edd0f883b became pre-candidate at term 1"}
{"level":"info","ts":"2024-12-09T18:28:40.919529Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8f28a26edd0f883b received MsgPreVoteResp from 8f28a26edd0f883b at term 1"}
{"level":"info","ts":"2024-12-09T18:28:40.919616Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8f28a26edd0f883b became candidate at term 2"}
{"level":"info","ts":"2024-12-09T18:28:40.919668Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8f28a26edd0f883b received MsgVoteResp from 8f28a26edd0f883b at term 2"}
{"level":"info","ts":"2024-12-09T18:28:40.919748Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8f28a26edd0f883b became leader at term 2"}
{"level":"info","ts":"2024-12-09T18:28:40.919792Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 8f28a26edd0f883b elected leader 8f28a26edd0f883b at term 2"}
{"level":"info","ts":"2024-12-09T18:28:40.928330Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"8f28a26edd0f883b","local-member-attributes":"{Name:minikube ClientURLs:[https://172.17.230.63:2379]}","request-path":"/0/members/8f28a26edd0f883b/attributes","cluster-id":"6057d4e182cc4f48","publish-timeout":"7s"}
{"level":"info","ts":"2024-12-09T18:28:40.929104Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-12-09T18:28:40.929477Z","caller":"etcdserver/server.go:2629","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-12-09T18:28:40.934142Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-12-09T18:28:40.939185Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"172.17.230.63:2379"}
{"level":"info","ts":"2024-12-09T18:28:40.939620Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"6057d4e182cc4f48","local-member-id":"8f28a26edd0f883b","cluster-version":"3.5"}
{"level":"info","ts":"2024-12-09T18:28:40.942177Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-12-09T18:28:40.931096Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-12-09T18:28:40.931139Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-12-09T18:28:40.947327Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-12-09T18:28:40.942531Z","caller":"etcdserver/server.go:2653","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-12-09T18:28:40.943068Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-12-09T18:28:40.950215Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"warn","ts":"2024-12-09T18:28:47.752876Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.394671ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/ttl-after-finished-controller\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-12-09T18:28:47.760172Z","caller":"traceutil/trace.go:171","msg":"trace[1478839881] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/ttl-after-finished-controller; range_end:; response_count:0; response_revision:294; }","duration":"117.698529ms","start":"2024-12-09T18:28:47.642445Z","end":"2024-12-09T18:28:47.760143Z","steps":["trace[1478839881] 'range keys from in-memory index tree'  (duration: 110.320739ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T18:28:48.421878Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"130.420826ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/kube-system\" ","response":"range_response_count:1 size:351"}
{"level":"info","ts":"2024-12-09T18:28:48.421963Z","caller":"traceutil/trace.go:171","msg":"trace[1614241564] range","detail":"{range_begin:/registry/namespaces/kube-system; range_end:; response_count:1; response_revision:300; }","duration":"130.519662ms","start":"2024-12-09T18:28:48.291425Z","end":"2024-12-09T18:28:48.421945Z","steps":["trace[1614241564] 'range keys from in-memory index tree'  (duration: 130.292681ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T18:28:48.422253Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"164.66315ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/generic-garbage-collector\" ","response":"range_response_count:1 size:216"}
{"level":"info","ts":"2024-12-09T18:28:48.422286Z","caller":"traceutil/trace.go:171","msg":"trace[39345785] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/generic-garbage-collector; range_end:; response_count:1; response_revision:300; }","duration":"164.699581ms","start":"2024-12-09T18:28:48.257576Z","end":"2024-12-09T18:28:48.422276Z","steps":["trace[39345785] 'range keys from in-memory index tree'  (duration: 164.587038ms)"],"step_count":1}
{"level":"info","ts":"2024-12-09T18:28:48.422771Z","caller":"traceutil/trace.go:171","msg":"trace[1599809698] transaction","detail":"{read_only:false; response_revision:301; number_of_response:1; }","duration":"117.403662ms","start":"2024-12-09T18:28:48.305355Z","end":"2024-12-09T18:28:48.422759Z","steps":["trace[1599809698] 'process raft request'  (duration: 117.29988ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T18:28:51.548482Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.840096ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-12-09T18:28:51.548644Z","caller":"traceutil/trace.go:171","msg":"trace[68306800] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:328; }","duration":"112.012794ms","start":"2024-12-09T18:28:51.436615Z","end":"2024-12-09T18:28:51.548628Z","steps":["trace[68306800] 'range keys from in-memory index tree'  (duration: 111.82693ms)"],"step_count":1}
{"level":"info","ts":"2024-12-09T18:38:41.313245Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":787}
{"level":"info","ts":"2024-12-09T18:38:41.319585Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":787,"took":"6.093181ms","hash":2989542269,"current-db-size-bytes":2588672,"current-db-size":"2.6 MB","current-db-size-in-use-bytes":2588672,"current-db-size-in-use":"2.6 MB"}
{"level":"info","ts":"2024-12-09T18:38:41.319635Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2989542269,"revision":787,"compact-revision":-1}
{"level":"info","ts":"2024-12-09T18:43:41.327985Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1072}
{"level":"info","ts":"2024-12-09T18:43:41.333208Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1072,"took":"4.895699ms","hash":2710491480,"current-db-size-bytes":2588672,"current-db-size":"2.6 MB","current-db-size-in-use-bytes":1712128,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-12-09T18:43:41.333277Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2710491480,"revision":1072,"compact-revision":787}
{"level":"info","ts":"2024-12-09T18:48:41.336356Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1352}
{"level":"info","ts":"2024-12-09T18:48:41.340177Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1352,"took":"3.533169ms","hash":1845315445,"current-db-size-bytes":2588672,"current-db-size":"2.6 MB","current-db-size-in-use-bytes":1703936,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-12-09T18:48:41.340226Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1845315445,"revision":1352,"compact-revision":1072}


==> kernel <==
 18:52:15 up 25 min,  0 users,  load average: 1.43, 0.73, 0.36
Linux minikube 5.10.207 #1 SMP Tue Sep 3 21:45:30 UTC 2024 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2023.02.9"


==> kube-apiserver [8a9967351e38] <==
I1209 18:28:43.264861       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1209 18:28:43.264946       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1209 18:28:43.265111       1 controller.go:119] Starting legacy_token_tracking_controller
I1209 18:28:43.265128       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1209 18:28:43.268247       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1209 18:28:43.268495       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1209 18:28:43.269677       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1209 18:28:43.269861       1 aggregator.go:169] waiting for initial CRD sync...
I1209 18:28:43.270039       1 local_available_controller.go:156] Starting LocalAvailability controller
I1209 18:28:43.270132       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1209 18:28:43.270292       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1209 18:28:43.308980       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1209 18:28:43.312950       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1209 18:28:43.313528       1 controller.go:142] Starting OpenAPI controller
I1209 18:28:43.313662       1 controller.go:90] Starting OpenAPI V3 controller
I1209 18:28:43.313844       1 naming_controller.go:294] Starting NamingConditionController
I1209 18:28:43.313981       1 establishing_controller.go:81] Starting EstablishingController
I1209 18:28:43.314044       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1209 18:28:43.314123       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1209 18:28:43.314189       1 crd_finalizer.go:269] Starting CRDFinalizer
I1209 18:28:43.314254       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1209 18:28:43.314312       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1209 18:28:43.362191       1 shared_informer.go:320] Caches are synced for node_authorizer
I1209 18:28:43.364271       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1209 18:28:43.364341       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1209 18:28:43.364352       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1209 18:28:43.364451       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1209 18:28:43.368762       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1209 18:28:43.374255       1 cache.go:39] Caches are synced for LocalAvailability controller
I1209 18:28:43.378961       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1209 18:28:43.379563       1 shared_informer.go:320] Caches are synced for configmaps
I1209 18:28:43.381738       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1209 18:28:43.381821       1 policy_source.go:224] refreshing policies
I1209 18:28:43.406419       1 controller.go:615] quota admission added evaluator for: namespaces
I1209 18:28:43.417604       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1209 18:28:43.417691       1 aggregator.go:171] initial CRD sync complete...
I1209 18:28:43.417700       1 autoregister_controller.go:144] Starting autoregister controller
I1209 18:28:43.417706       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1209 18:28:43.417711       1 cache.go:39] Caches are synced for autoregister controller
I1209 18:28:43.478194       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1209 18:28:44.285609       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1209 18:28:44.295188       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1209 18:28:44.295262       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1209 18:28:45.254691       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1209 18:28:45.314470       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1209 18:28:45.422116       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1209 18:28:45.431218       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [172.17.230.63]
I1209 18:28:45.432265       1 controller.go:615] quota admission added evaluator for: endpoints
I1209 18:28:45.442145       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1209 18:28:46.379667       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I1209 18:28:46.686234       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1209 18:28:46.717962       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1209 18:28:46.735257       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I1209 18:28:51.917407       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I1209 18:28:52.062509       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I1209 18:30:19.558749       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.103.100.64"}
I1209 18:30:19.576351       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.105.160.29"}
I1209 18:30:19.603778       1 controller.go:615] quota admission added evaluator for: jobs.batch
I1209 18:32:56.746269       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes-service" clusterIPs={"IPv4":"10.106.115.220"}
I1209 18:35:11.643578       1 controller.go:615] quota admission added evaluator for: ingresses.networking.k8s.io


==> kube-controller-manager [e3e32081aa4c] <==
I1209 18:28:51.212459       1 shared_informer.go:320] Caches are synced for TTL
I1209 18:28:51.213585       1 shared_informer.go:320] Caches are synced for daemon sets
I1209 18:28:51.228815       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 18:28:51.228906       1 shared_informer.go:320] Caches are synced for ReplicationController
I1209 18:28:51.262847       1 shared_informer.go:320] Caches are synced for GC
I1209 18:28:51.317054       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I1209 18:28:51.317353       1 shared_informer.go:320] Caches are synced for disruption
I1209 18:28:51.318498       1 shared_informer.go:320] Caches are synced for resource quota
I1209 18:28:51.325583       1 shared_informer.go:320] Caches are synced for resource quota
I1209 18:28:51.367546       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1209 18:28:51.737152       1 shared_informer.go:320] Caches are synced for garbage collector
I1209 18:28:51.737228       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1209 18:28:51.806275       1 shared_informer.go:320] Caches are synced for garbage collector
I1209 18:28:52.027431       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 18:28:52.260256       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="335.967947ms"
I1209 18:28:52.278536       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="18.213575ms"
I1209 18:28:52.278843       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="91.558µs"
I1209 18:28:52.297758       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="93.839µs"
I1209 18:28:54.069367       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="804.01µs"
I1209 18:28:56.178754       1 node_lifecycle_controller.go:1055] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I1209 18:29:32.688418       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="15.94797ms"
I1209 18:29:32.688806       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="66.098µs"
I1209 18:30:19.609428       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="0s"
I1209 18:30:19.627354       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="0s"
I1209 18:30:19.638521       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I1209 18:30:19.641441       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-bc57996ff" duration="36.713908ms"
I1209 18:30:19.642481       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I1209 18:30:19.644318       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I1209 18:30:19.651158       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I1209 18:30:19.655347       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I1209 18:30:19.666468       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I1209 18:30:19.668498       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-bc57996ff" duration="26.948881ms"
I1209 18:30:19.684151       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I1209 18:30:19.688996       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-bc57996ff" duration="20.373633ms"
I1209 18:30:19.689107       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-bc57996ff" duration="80.697µs"
I1209 18:30:19.693306       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I1209 18:30:27.636120       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I1209 18:30:27.646694       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I1209 18:30:28.770656       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I1209 18:30:28.786777       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I1209 18:30:29.779579       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I1209 18:30:29.786318       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I1209 18:30:29.791673       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I1209 18:30:29.795385       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I1209 18:30:29.802185       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I1209 18:30:29.808069       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I1209 18:30:49.287340       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 18:31:01.854622       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-bc57996ff" duration="35.799µs"
I1209 18:31:15.561446       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-bc57996ff" duration="12.533399ms"
I1209 18:31:15.561644       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-bc57996ff" duration="169.956µs"
I1209 18:31:19.467878       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 18:31:32.554177       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kubernetes-homework-7668f8bd44" duration="26.298773ms"
I1209 18:31:32.564579       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kubernetes-homework-7668f8bd44" duration="9.578236ms"
I1209 18:31:32.565390       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kubernetes-homework-7668f8bd44" duration="22.472µs"
I1209 18:32:04.185280       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kubernetes-homework-7668f8bd44" duration="6.901567ms"
I1209 18:32:04.185430       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kubernetes-homework-7668f8bd44" duration="39.927µs"
I1209 18:32:20.563133       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 18:37:26.164423       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 18:42:31.594134       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 18:47:36.858459       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [a3addd9728d4] <==
I1209 18:28:53.114502       1 server_linux.go:66] "Using iptables proxy"
E1209 18:28:53.141299       1 proxier.go:734] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-24: Error: Could not process rule: Operation not supported
	add table ip kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^
 >
E1209 18:28:53.159843       1 proxier.go:734] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-25: Error: Could not process rule: Operation not supported
	add table ip6 kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^^
 >
I1209 18:28:53.178804       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["172.17.230.63"]
E1209 18:28:53.179073       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1209 18:28:53.254739       1 server_linux.go:146] "No iptables support for family" ipFamily="IPv6"
I1209 18:28:53.254797       1 server.go:245] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I1209 18:28:53.254839       1 server_linux.go:169] "Using iptables Proxier"
I1209 18:28:53.274942       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1209 18:28:53.275292       1 server.go:483] "Version info" version="v1.31.0"
I1209 18:28:53.275336       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1209 18:28:53.277431       1 config.go:197] "Starting service config controller"
I1209 18:28:53.277492       1 shared_informer.go:313] Waiting for caches to sync for service config
I1209 18:28:53.277514       1 config.go:104] "Starting endpoint slice config controller"
I1209 18:28:53.277518       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1209 18:28:53.277975       1 config.go:326] "Starting node config controller"
I1209 18:28:53.277995       1 shared_informer.go:313] Waiting for caches to sync for node config
I1209 18:28:53.380189       1 shared_informer.go:320] Caches are synced for node config
I1209 18:28:53.380231       1 shared_informer.go:320] Caches are synced for service config
I1209 18:28:53.380325       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [41bcb59bf522] <==
E1209 18:28:43.439956       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1209 18:28:43.440545       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1209 18:28:43.440721       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1209 18:28:43.441862       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1209 18:28:43.441903       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1209 18:28:43.443015       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1209 18:28:43.443097       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1209 18:28:43.444038       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1209 18:28:43.444104       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1209 18:28:43.444296       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1209 18:28:43.444722       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1209 18:28:43.444750       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1209 18:28:43.444822       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1209 18:28:43.444843       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E1209 18:28:43.444327       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1209 18:28:43.446930       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1209 18:28:43.446961       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1209 18:28:43.447021       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1209 18:28:43.447056       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1209 18:28:43.447200       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1209 18:28:43.447223       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1209 18:28:43.447342       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1209 18:28:43.447536       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1209 18:28:43.447402       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1209 18:28:43.447588       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1209 18:28:43.447447       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1209 18:28:43.447609       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1209 18:28:43.447468       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1209 18:28:43.447639       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1209 18:28:44.358180       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1209 18:28:44.358345       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1209 18:28:44.421500       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1209 18:28:44.422013       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1209 18:28:44.458792       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1209 18:28:44.458872       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1209 18:28:44.474775       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1209 18:28:44.474920       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1209 18:28:44.474846       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1209 18:28:44.475047       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1209 18:28:44.482240       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1209 18:28:44.482421       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1209 18:28:44.504297       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1209 18:28:44.504346       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1209 18:28:44.601550       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1209 18:28:44.601823       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1209 18:28:44.634651       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1209 18:28:44.635152       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1209 18:28:44.709883       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1209 18:28:44.709944       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1209 18:28:44.729237       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1209 18:28:44.729325       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1209 18:28:44.763020       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1209 18:28:44.763093       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1209 18:28:44.806947       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1209 18:28:44.807017       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1209 18:28:44.877185       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1209 18:28:44.877421       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1209 18:28:44.921388       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1209 18:28:44.921478       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
I1209 18:28:47.931211       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Dec 09 18:40:46 minikube kubelet[2180]: E1209 18:40:46.819843    2180 iptables.go:577] "Could not set up iptables canary" err=<
Dec 09 18:40:46 minikube kubelet[2180]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Dec 09 18:40:46 minikube kubelet[2180]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Dec 09 18:40:46 minikube kubelet[2180]:         Perhaps ip6tables or your kernel needs to be upgraded.
Dec 09 18:40:46 minikube kubelet[2180]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Dec 09 18:41:46 minikube kubelet[2180]: E1209 18:41:46.819987    2180 iptables.go:577] "Could not set up iptables canary" err=<
Dec 09 18:41:46 minikube kubelet[2180]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Dec 09 18:41:46 minikube kubelet[2180]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Dec 09 18:41:46 minikube kubelet[2180]:         Perhaps ip6tables or your kernel needs to be upgraded.
Dec 09 18:41:46 minikube kubelet[2180]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Dec 09 18:42:46 minikube kubelet[2180]: E1209 18:42:46.818596    2180 iptables.go:577] "Could not set up iptables canary" err=<
Dec 09 18:42:46 minikube kubelet[2180]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Dec 09 18:42:46 minikube kubelet[2180]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Dec 09 18:42:46 minikube kubelet[2180]:         Perhaps ip6tables or your kernel needs to be upgraded.
Dec 09 18:42:46 minikube kubelet[2180]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Dec 09 18:43:46 minikube kubelet[2180]: E1209 18:43:46.821434    2180 iptables.go:577] "Could not set up iptables canary" err=<
Dec 09 18:43:46 minikube kubelet[2180]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Dec 09 18:43:46 minikube kubelet[2180]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Dec 09 18:43:46 minikube kubelet[2180]:         Perhaps ip6tables or your kernel needs to be upgraded.
Dec 09 18:43:46 minikube kubelet[2180]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Dec 09 18:44:46 minikube kubelet[2180]: E1209 18:44:46.819970    2180 iptables.go:577] "Could not set up iptables canary" err=<
Dec 09 18:44:46 minikube kubelet[2180]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Dec 09 18:44:46 minikube kubelet[2180]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Dec 09 18:44:46 minikube kubelet[2180]:         Perhaps ip6tables or your kernel needs to be upgraded.
Dec 09 18:44:46 minikube kubelet[2180]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Dec 09 18:45:46 minikube kubelet[2180]: E1209 18:45:46.819726    2180 iptables.go:577] "Could not set up iptables canary" err=<
Dec 09 18:45:46 minikube kubelet[2180]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Dec 09 18:45:46 minikube kubelet[2180]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Dec 09 18:45:46 minikube kubelet[2180]:         Perhaps ip6tables or your kernel needs to be upgraded.
Dec 09 18:45:46 minikube kubelet[2180]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Dec 09 18:46:46 minikube kubelet[2180]: E1209 18:46:46.819164    2180 iptables.go:577] "Could not set up iptables canary" err=<
Dec 09 18:46:46 minikube kubelet[2180]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Dec 09 18:46:46 minikube kubelet[2180]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Dec 09 18:46:46 minikube kubelet[2180]:         Perhaps ip6tables or your kernel needs to be upgraded.
Dec 09 18:46:46 minikube kubelet[2180]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Dec 09 18:47:46 minikube kubelet[2180]: E1209 18:47:46.818159    2180 iptables.go:577] "Could not set up iptables canary" err=<
Dec 09 18:47:46 minikube kubelet[2180]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Dec 09 18:47:46 minikube kubelet[2180]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Dec 09 18:47:46 minikube kubelet[2180]:         Perhaps ip6tables or your kernel needs to be upgraded.
Dec 09 18:47:46 minikube kubelet[2180]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Dec 09 18:48:46 minikube kubelet[2180]: E1209 18:48:46.819738    2180 iptables.go:577] "Could not set up iptables canary" err=<
Dec 09 18:48:46 minikube kubelet[2180]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Dec 09 18:48:46 minikube kubelet[2180]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Dec 09 18:48:46 minikube kubelet[2180]:         Perhaps ip6tables or your kernel needs to be upgraded.
Dec 09 18:48:46 minikube kubelet[2180]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Dec 09 18:49:46 minikube kubelet[2180]: E1209 18:49:46.819933    2180 iptables.go:577] "Could not set up iptables canary" err=<
Dec 09 18:49:46 minikube kubelet[2180]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Dec 09 18:49:46 minikube kubelet[2180]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Dec 09 18:49:46 minikube kubelet[2180]:         Perhaps ip6tables or your kernel needs to be upgraded.
Dec 09 18:49:46 minikube kubelet[2180]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Dec 09 18:50:46 minikube kubelet[2180]: E1209 18:50:46.820067    2180 iptables.go:577] "Could not set up iptables canary" err=<
Dec 09 18:50:46 minikube kubelet[2180]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Dec 09 18:50:46 minikube kubelet[2180]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Dec 09 18:50:46 minikube kubelet[2180]:         Perhaps ip6tables or your kernel needs to be upgraded.
Dec 09 18:50:46 minikube kubelet[2180]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Dec 09 18:51:46 minikube kubelet[2180]: E1209 18:51:46.820884    2180 iptables.go:577] "Could not set up iptables canary" err=<
Dec 09 18:51:46 minikube kubelet[2180]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Dec 09 18:51:46 minikube kubelet[2180]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Dec 09 18:51:46 minikube kubelet[2180]:         Perhaps ip6tables or your kernel needs to be upgraded.
Dec 09 18:51:46 minikube kubelet[2180]:  > table="nat" chain="KUBE-KUBELET-CANARY"


==> storage-provisioner [7a1f24d56a3d] <==
I1209 18:28:59.134301       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1209 18:28:59.145790       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1209 18:28:59.145993       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1209 18:28:59.160052       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1209 18:28:59.160391       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_24af69dc-0b79-4260-aa7e-58453d8141ea!
I1209 18:28:59.162066       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"cf6c94e3-6278-42cc-8c20-874ee7a6544f", APIVersion:"v1", ResourceVersion:"399", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_24af69dc-0b79-4260-aa7e-58453d8141ea became leader
I1209 18:28:59.262483       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_24af69dc-0b79-4260-aa7e-58453d8141ea!

